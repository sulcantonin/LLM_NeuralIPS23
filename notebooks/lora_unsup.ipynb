{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning LLM for Particle Accelerators\n",
    "\n",
    "This code is inspired by \n",
    "https://medium.com/@ud.chandra/instruction-fine-tuning-llama-2-with-pefts-qlora-method-d6a801ebb19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuXIFTFapAMI",
    "outputId": "1f964ae2-e63c-4f7f-aac1-3bb5f427c9b0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install --upgrade accelerate\n",
    "!pip install --upgrade datasets\n",
    "!pip install --upgrade bitsandbytes\n",
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade peft\n",
    "!pip install --upgrade deepspeed\n",
    "!pip install --upgrade optimum\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from glob import glob\n",
    "from unidecode import unidecode\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from simcse import SimCSE\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../code\")\n",
    "import os\n",
    "\n",
    "from core import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "def prompt_formatter(question, answer = \"\"):\n",
    "    return f'### Human:\\n{question}\\n### Assistant:\\n{answer}'\n",
    "    # return f'USER: {question}\\nASSISTANT: {answer}</s>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "b1528cf7-b647-415e-9a08-fed1f2b50c72"
   },
   "outputs": [],
   "source": [
    "IMG_TAG = re.compile(\"<image:.*>\")\n",
    "NEW_LINE = re.compile(r\"-\\n\")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "smoothen = lambda l, N=12: np.convolve(l, np.ones(N) / N, mode=\"valid\")\n",
    "\n",
    "# model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "# model_id = 'lmsys/vicuna-7b-v1.5-16k'\n",
    "# model_id = \"lmsys/vicuna-7b-v1.5\"\n",
    "model_id = \"lmsys/vicuna-7b-v1.5-16k\"\n",
    "# model_id = \"openlm-research/open_llama_3b\"\n",
    "# model_id = 'openlm-research/open_llama_7b'\n",
    "# model_id = 'NousResearch/Nous-Hermes-llama-2-7b'\n",
    "# model_id = \"NousResearch/Nous-Hermes-13b\"\n",
    "# model_id = \"NousResearch/Llama-2-13b-chat-hf\"\n",
    "# model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# model_id = \"tiiuae/falcon-7b\"\n",
    "# model_id = 'tiiuae/falcon-7b-instruct'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=True, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # load_in_8bit = True,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    # target_modules=[\"query_key_value\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# viz https://huggingface.co/docs/transformers/v4.32.0/en/perf_train_gpu_one#optimizer-choice\n",
    "# using flash attention - not enabled, because there is a problem with saving a training loop\n",
    "# model = model.to_bettertransformer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WoJ79VCMBuT"
   },
   "outputs": [],
   "source": [
    "def prepare_pretrain(filename, crit = lambda text : True):\n",
    "'''\n",
    "This script prepares the raw text (preprocessed MMD from nougat) and removes some redundant parts for the unsupervised \n",
    "fine-tuning\n",
    "'''\n",
    "    train_sentences_ = []\n",
    "    with gzip.open(filename,'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        text_set = set()\n",
    "        for qa in tqdm(x):\n",
    "            text = qa['metadata']['text']\n",
    "            if crit(text):\n",
    "                if not text in text_set:\n",
    "                    text = re.sub('^\\s+','',text)\n",
    "                    text = re.sub('\\s+$','',text)\n",
    "                    text = re.sub('#','', text)\n",
    "                    train_sentences_.append(text)\n",
    "                    text_set.update([text])\n",
    "    return train_sentences_\n",
    "\n",
    "def prepare_finetune(filename):\n",
    "'''\n",
    "This script prepares the raw text (preprocessed MMD from nougat) and removes some redundant parts for the supervised \n",
    "fine-tuning\n",
    "'''\n",
    "    train_sentences_ = []\n",
    "    with gzip.open(filename,'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        for x_ in tqdm(x):\n",
    "            for qa in x_['pairs']:\n",
    "                q = qa['question']\n",
    "                a = qa['answer']\n",
    "                \n",
    "                # removing redundant white spaces\n",
    "                q = re.sub('\\s+$','',q)\n",
    "                a = re.sub('\\s+$','',a)\n",
    "                \n",
    "                # removing redunadnt white spaces (end)\n",
    "                q = re.sub('^\\s+','',q)\n",
    "                a = re.sub('^\\s+','',a)\n",
    "                \n",
    "                # removing \"answer\" texts generated by vicuna\n",
    "                a = re.sub('^\\s*Answer:','',a)\n",
    "                a = re.sub('^\\s*Answers:','',a)\n",
    "                a = re.sub('^\\s*A:','',a)\n",
    "                \n",
    "                # removing \"question\" texts generated by vicuna\n",
    "                a = re.sub('^\\s*Question:','',a)\n",
    "                a = re.sub('^\\s*Questions:','',a)\n",
    "                a = re.sub('^\\s*Q:','',a)\n",
    "                \n",
    "                # removing numbers in front 1.\n",
    "                a = re.sub('^\\d+\\.\\s*','',a)\n",
    "                q = re.sub('^\\d+\\.\\s*','',q)\n",
    "                \n",
    "                train_sentences_.append(prompt_formatter(q,a))\n",
    "    return train_sentences_\n",
    "\n",
    "data_folder = '../data/'\n",
    "sentences = []\n",
    "sentences.extend(prepare_pretrain(data_folder + 'arxiv_pretrain.pickle.gzip', lambda x : True))# 'DESY' in x))\n",
    "sentences.extend(prepare_pretrain(data_folder + 'books_accelerators_pretrain.pickle.gzip', lambda x : True))\n",
    "sentences.extend(prepare_finetune(data_folder + 'books_accelerators_qa_vicuna.pickle.gzip'))\n",
    "sentences.extend(prepare_pretrain(data_folder + 'proc_pretrain.pickle.gzip', lambda x : True))# 'DESY' in x))\n",
    "sentences.extend(prepare_finetune(data_folder + 'proc_qa_vicuna.pickle.gzip'))\n",
    "\n",
    "MAX_LEN = tokenizer.model_max_length\n",
    "MIN_LEN = 16\n",
    "train_sentences = []\n",
    "for sent in tqdm(sentences):  \n",
    "    LEN = len(tokenizer.encode(sent))\n",
    "    if LEN <= MAX_LEN and LEN > MIN_LEN:\n",
    "        train_sentences.append(unidecode(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences), len(train_sentences))\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_dict({\"text\": train_sentences})\n",
    "dataset = dataset.shuffle(seed = 42)\n",
    "data = datasets.DatasetDict({\"train\": dataset})\n",
    "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq0nX33BmfaC",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer = transformers.Trainer(\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],    \n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    dataset_text_field = 'text',\n",
    "    args=transformers.TrainingArguments(\n",
    "        # lr_scheduler_type = 'cosine', \n",
    "        per_device_train_batch_size=2,  # 1,\n",
    "        gradient_accumulation_steps=16, # 4\n",
    "        warmup_ratio=0.05,\n",
    "        num_train_epochs=4,\n",
    "        # max_steps=100,\n",
    "        auto_find_batch_size=True,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"outputs/human_assistant_prompt_all_papers\",\n",
    "        save_steps = 100,\n",
    "        # optim = 'paged_adamw_8bit'\n",
    "        # optim= \"paged_adamw_32bit\"\n",
    "        # optim= \"paged_adamw_8bit\"\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False  # silennce the warnings. Please re-enable for inference!\n",
    "trainer.train(resume_from_checkpoint = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([t[\"loss\"] for t in trainer.state.log_history if \"loss\" in t])\n",
    "plt.plot(l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
